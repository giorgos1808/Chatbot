{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb45e06-799b-4bd6-8d0c-c493f23903a9",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3183116-3abb-4676-96a3-48d944975949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Input, Concatenate, Lambda, ZeroPadding2D, Add\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import typing\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932214bd-cfda-4a17-8304-5395008147dd",
   "metadata": {},
   "source": [
    "# Setting TensorFlow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dadfc-2c58-4a4d-87ef-743d4098e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f20625-820f-412b-bd53-cb3bc9ab5a09",
   "metadata": {},
   "source": [
    "# Initializing WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4227a-4339-45d6-98bb-52820fd11296",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b40d2-a79d-4b40-b159-09aab35ed644",
   "metadata": {},
   "source": [
    "# Getting the absolute path of the current file's parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c93c3-7005-400b-9610-e9ed09d7bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(__file__).parent.absolute()\n",
    "str_path = str(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f79b0c-e438-44e4-ae36-62e4a12d3366",
   "metadata": {},
   "source": [
    "# Function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7454c8-7c30-4cc1-ade4-527c5665f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset = 'dataset_22.json'):\n",
    "    intents = json.loads(open(dataset, encoding='utf-8').read())\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_letters = ['?', '!', '.', \",\"]\n",
    "\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            word_list = nltk.word_tokenize(pattern)\n",
    "            words.extend(word_list)\n",
    "            documents.append((word_list, intent['tag']))\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "\n",
    "    #Lexicon of words and classes\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "\n",
    "    words = sorted(set(words))\n",
    "    classes = sorted(set(classes))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    ignore_letters = ['?', '!', '.', \",\"]\n",
    "\n",
    "    bag = []\n",
    "    for document in documents:\n",
    "        word_patterns = document[0]\n",
    "        # word_patterns -> sentence with lower words + lemmatize\n",
    "        word_patterns = [lemmatizer.lemmatize((word.lower())) for word in word_patterns]\n",
    "        #word_patterns_without_stopwords = [lemmatizer.lemmatize(word) for word in word_patterns if word not in stopwords]\n",
    "        #word_patt = [lemmatizer.lemmatize(word) for word in word_patterns_without_stopwords if word not in ignore_letters]\n",
    "        word_patterns_ignore_letters = [lemmatizer.lemmatize(word) for word in word_patterns if word not in ignore_letters]\n",
    "        bag.append(word_patterns_ignore_letters)\n",
    "\n",
    "    class_pattern = []\n",
    "    for document in documents:\n",
    "        classes = document[1]\n",
    "        class_pattern.append(classes)\n",
    "\n",
    "    return bag, class_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a49da-1ca7-4f79-985c-8464fd006092",
   "metadata": {},
   "source": [
    "# Function for creating TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb533e2-273e-4763-8014-7ba21749f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield TaggedDocument(list_of_words, [i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bf2ab-0717-4ff0-a23e-7cddc3cfabce",
   "metadata": {},
   "source": [
    "# Function to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd8e45-8bfb-4944-be4e-e6c38e904283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data,vc,ep):\n",
    "    data_for_training = list(tagged_document(data))\n",
    "    model = Doc2Vec(vector_size=vc,min_count=2, epochs=ep)\n",
    "    model.build_vocab(data_for_training)\n",
    "    model.train(data_for_training,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14a8cc-82fb-49d7-ba15-40f708556632",
   "metadata": {},
   "source": [
    "# Function for training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953221d-d154-4542-863b-4ac5dd2464a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tain_vec(bag, voc_size1, epochs):\n",
    "    model_x = build_model(bag,voc_size1,epochs)\n",
    "    train_x_dv = []\n",
    "    for i in range(len(model_x.dv)):\n",
    "        train_x_dv.append(model_x.dv[i])\n",
    "\n",
    "    return train_x_dv, model_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e3b46-2ec7-4f75-a96a-097085b11d2c",
   "metadata": {},
   "source": [
    "# Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20ed6a-4780-4ce8-b2cf-2bc00fe7c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(bag, class_pattern, size1, size2, width, heigth, channel, num_ep):\n",
    "    train_x_dv, model_x = tain_vec(bag[size1:size2], width*heigth*channel, num_ep)\n",
    "\n",
    "    num_of_classes = []\n",
    "    for i in range(size1,size2):\n",
    "        if class_pattern[i] not in num_of_classes:\n",
    "            num_of_classes.append(class_pattern[i])\n",
    "\n",
    "    train_y = []\n",
    "    for i in range(len(num_of_classes)):\n",
    "        train_y.append(i)\n",
    "\n",
    "    y_train = []\n",
    "    for i in range(size1,size2):\n",
    "        for j in range(len(num_of_classes)):\n",
    "            if class_pattern[i] == num_of_classes[j]:\n",
    "                y_train.append(train_y[j])\n",
    "\n",
    "    x_i = []\n",
    "    for i in range(size1,size2):\n",
    "        x = np.reshape(train_x_dv[i], (width, heigth, channel))\n",
    "        x_i.append(x)\n",
    "    x_2 = np.array(x_i)\n",
    "\n",
    "    return x_2, y_train, model_x, num_of_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc9ed-7321-4725-a41e-b3dd784a9704",
   "metadata": {},
   "source": [
    "# LeNet5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42fcd9-5d53-47e4-b3a3-f38a48173b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_5(max_lebels = 11):\n",
    "    model = Sequential()\n",
    "\n",
    "    # C1: (None,32,32,1) -> (None,28,28,6).\n",
    "    model.add(\n",
    "        Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(32, 32, 1), padding='valid'))\n",
    "\n",
    "    # P1: (None,28,28,6) -> (None,14,14,6).\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    # C2: (None,14,14,6) -> (None,10,10,16).\n",
    "    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "    # P2: (None,10,10,16) -> (None,5,5,16).\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "    # Flatten: (None,5,5,16) -> (None, 400).\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # FC1: (None, 400) -> (None,120).\n",
    "    model.add(Dense(120, activation='tanh'))\n",
    "\n",
    "    # FC2: (None,120) -> (None,84).\n",
    "    model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "    # FC3: (None,84) -> (None,10).\n",
    "    model.add(Dense(max_lebels, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcdf6a-56d4-47d4-9bc3-2e4ca62e0cee",
   "metadata": {},
   "source": [
    "# Grouped convolution function for AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffb1d3-dd5b-4324-9129-798bc56f0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_conv(input_val, name, half, filters, kernel_size, strides=1, padding='valid'):\n",
    "    \"\"\"\n",
    "        Performs a grouped convolution.\n",
    "\n",
    "        Parameters:\n",
    "        -input_val: previous layer.\n",
    "        -name: name of the convolution.\n",
    "        -half: Number of channels for each convolution.\n",
    "        -filters: Number of filters for each convolution.\n",
    "        -kernel_size: Kernel size used for each convolution.\n",
    "        -strides: stride. Default value is 1.\n",
    "        -padding: 'valid'(default) or 'same'.\n",
    "\n",
    "        Returns:\n",
    "        -conv: concatenation of the 2 previous convolution layer.\n",
    "\n",
    "    \"\"\"\n",
    "    input_val_1 = Lambda(lambda x: x[:, :, :, :half])(input_val)\n",
    "    input_val_2 = Lambda(lambda x: x[:, :, :, half:])(input_val)\n",
    "\n",
    "    conv_1 = Conv2D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation='relu',\n",
    "                    name=name + '_1')(input_val_1)\n",
    "\n",
    "    conv_2 = Conv2D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    activation='relu',\n",
    "                    name=name + '_2')(input_val_2)\n",
    "\n",
    "    conv = Concatenate(name=name)([conv_1, conv_2])\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943785f-fd17-4497-a78d-ac5f26304abb",
   "metadata": {},
   "source": [
    "# AlexNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541b916-0287-437a-8791-0e18d95631a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet(max_lebels = 11):\n",
    "    x = Input((227, 227, 3))\n",
    "\n",
    "    conv1 = Conv2D(filters=96,\n",
    "                   kernel_size=(11, 11),\n",
    "                   strides=4,\n",
    "                   activation='relu',\n",
    "                   name='conv1')(x)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv1)\n",
    "\n",
    "    conv2 = grouped_conv(input_val=pool1,\n",
    "                         name='conv2',\n",
    "                         half=48,\n",
    "                         filters=128,\n",
    "                         kernel_size=5,\n",
    "                         padding='same')\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters=384,\n",
    "                   kernel_size=(3, 3),\n",
    "                   padding='same',\n",
    "                   activation='relu',\n",
    "                   name='conv3')(pool2)\n",
    "\n",
    "    conv4 = grouped_conv(input_val=conv3,\n",
    "                         name='conv4',\n",
    "                         half=192,\n",
    "                         filters=192,\n",
    "                         kernel_size=3,\n",
    "                         padding='same')\n",
    "\n",
    "    conv5 = grouped_conv(input_val=conv4,\n",
    "                         name='conv5',\n",
    "                         half=192,\n",
    "                         filters=128,\n",
    "                         kernel_size=3,\n",
    "                         padding='same')\n",
    "\n",
    "    pool5 = MaxPooling2D(pool_size=3,\n",
    "                         strides=2)(conv5)\n",
    "\n",
    "    flatten = Flatten()(pool5)\n",
    "\n",
    "    fc6 = Dense(4096, activation='relu', name='fc6')(flatten)\n",
    "\n",
    "    fc7 = Dense(4096, activation='relu', name='fc7')(fc6)\n",
    "\n",
    "    fc8 = Dense(max_lebels, activation='softmax', name='fc8')(fc7)\n",
    "\n",
    "    model = Model(inputs=x, outputs=fc8)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b0ab3-78e9-47d9-b575-1c6f5476b2f7",
   "metadata": {},
   "source": [
    "# Block function for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cffd1-dc00-4219-9722-0616cd7426a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(X: tf.Tensor, kernel_size: int, filters: typing.List[int], stage_no: int, block_name: str, is_conv_layer: bool = False, stride: int = 2) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Block for residual network.\n",
    "    Arguments:\n",
    "    X             -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    kernel_size   -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters       -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage_no      -- integer, used to name the layers, depending on their position in the network\n",
    "    block_name    -- string/character, used to name the layers, depending on their position in the network\n",
    "    is_conv_layer -- to identiy if identity downsample is needed\n",
    "    stride        -- integer specifying the stride to be used\n",
    "\n",
    "    Returns:\n",
    "    X             -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # names\n",
    "    conv_name_base = \"res\" + str(stage_no) + block_name + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage_no) + block_name + \"_branch\"\n",
    "\n",
    "    # filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # save the input value for shortcut.\n",
    "    X_shortcut = X\n",
    "\n",
    "    #  First component\n",
    "    # NOTE: if conv_layer, you need to do downsampling\n",
    "    X = Conv2D(\n",
    "        filters=F1,\n",
    "        kernel_size=(1, 1),\n",
    "        strides=(stride, stride) if is_conv_layer else (1, 1),\n",
    "        padding=\"valid\",\n",
    "        name=conv_name_base + \"2a\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + \"2a\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    # Second component\n",
    "    X = Conv2D(\n",
    "        filters=F2,\n",
    "        kernel_size=(kernel_size, kernel_size),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        name=conv_name_base + \"2b\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + \"2b\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    # Third component\n",
    "    X = Conv2D(\n",
    "        filters=F3,\n",
    "        kernel_size=(1, 1),\n",
    "        strides=(1, 1),\n",
    "        padding=\"valid\",\n",
    "        name=conv_name_base + \"2c\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "    )(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + \"2c\")(X)\n",
    "\n",
    "    # NOTE: if is_conv_layer, you need to do downsampling the X_shortcut to match the output (X) channel\n",
    "    #       so it can be added together\n",
    "    if is_conv_layer:\n",
    "        X_shortcut = Conv2D(\n",
    "            filters=F3,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=(stride, stride),\n",
    "            padding=\"valid\",\n",
    "            name=conv_name_base + \"1\",\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "        )(X_shortcut)\n",
    "        X_shortcut = BatchNormalization(axis=3, name=bn_name_base + \"1\")(X_shortcut)\n",
    "\n",
    "    # Shortcut value\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation(\"relu\")(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce3539-9210-431b-8534-9859dd951f5e",
   "metadata": {},
   "source": [
    "# ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797c8c8-a72f-4f3c-8080-4a029368f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet(name: str, layers: typing.List[int], input_shape: typing.Tuple[int] = (64, 64, 3), classes: int = 6) -> Model:\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet architecture.\n",
    "    Arguments:\n",
    "    name        -- name of the architecture\n",
    "    layers      -- number of blocks per layer\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes     -- integer, number of classes\n",
    "    Returns:\n",
    "    model       -- a Model() instance in Keras\n",
    "    Model Architecture:\n",
    "    Resnet50:\n",
    "        CONV2D -> BATCHNORM -> RELU -> MAXPOOL  // conv1\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv2_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 3         // conv3_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 5         // conv4_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv5_x\n",
    "            -> AVGPOOL\n",
    "            -> TOPLAYER\n",
    "    Resnet101:\n",
    "        CONV2D -> BATCHNORM -> RELU -> MAXPOOL  // conv1\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv2_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 3         // conv3_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 22        // conv4_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv5_x\n",
    "            -> AVGPOOL\n",
    "            -> TOPLAYER\n",
    "    Resnet152:\n",
    "        CONV2D -> BATCHNORM -> RELU -> MAXPOOL  // conv1\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv2_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 7         // conv3_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 35        // conv4_x\n",
    "            -> CONVBLOCK -> IDBLOCK * 2         // conv5_x\n",
    "            -> AVGPOOL\n",
    "            -> TOPLAYER\n",
    "    \"\"\"\n",
    "\n",
    "    # get layers (layer1 is always the same so no need to provide)\n",
    "    layer2, layer3, layer4, layer5 = layers\n",
    "\n",
    "    # convert input shape into tensor\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # zero-padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # conv1\n",
    "    X = Conv2D(\n",
    "        filters = 64,\n",
    "        kernel_size = (7, 7),\n",
    "        strides = (2, 2),\n",
    "        name = \"conv1\",\n",
    "        kernel_initializer = \"glorot_uniform\",\n",
    "    )(X)\n",
    "    X = BatchNormalization(axis = 3, name = \"bn_conv1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = MaxPooling2D((3, 3), strides = (2, 2))(X)\n",
    "\n",
    "    # conv2_x\n",
    "    X = make_layer(X, layers = layer2, kernel_size = 3, filters = [64, 64, 256], stride = 1, stage_no = 2)\n",
    "\n",
    "    # conv3_x\n",
    "    X = make_layer(X, layers = layer3, kernel_size = 3, filters = [128, 128, 512], stride = 2, stage_no = 3)\n",
    "\n",
    "    # conv4_x\n",
    "    X = make_layer(X, layers = layer4, kernel_size = 3, filters = [256, 256, 1024], stride = 2, stage_no = 4)\n",
    "\n",
    "    # conv5_x\n",
    "    X = make_layer(X, layers = layer5, kernel_size = 3, filters = [512, 512, 2048], stride = 1, stage_no = 5)\n",
    "\n",
    "    # average pooling\n",
    "    X = AveragePooling2D((2, 2), name = \"avg_pool\")(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(\n",
    "        classes,\n",
    "        activation = \"softmax\",\n",
    "        name=\"fc\" + str(classes),\n",
    "        kernel_initializer = \"glorot_uniform\"\n",
    "    )(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef50da-957a-4685-be5f-c99a354aadc6",
   "metadata": {},
   "source": [
    "# Make layer function for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6c170-a253-4624-82c8-7d8ba1dba464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(X: tf.Tensor, layers: int, kernel_size: int, filters: typing.List[int], stride: int, stage_no: int) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Method to create one conv-identity layer for ResNet.\n",
    "    Arguments:\n",
    "    X           -- input tensor\n",
    "    layers      -- number of blocks per layer\n",
    "    kernel_size -- size of the kernel for the block\n",
    "    filters     -- number of filters/channels\n",
    "    stride      -- number of stride for downsampling the input\n",
    "    stage_no    -- stage number just to name the layer\n",
    "    Returns:\n",
    "    X           -- output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # create convolution block\n",
    "    X = block(\n",
    "        X,\n",
    "        kernel_size = kernel_size,\n",
    "        filters = filters,\n",
    "        stage_no = stage_no,\n",
    "        block_name = \"a\",\n",
    "        is_conv_layer = True,\n",
    "        stride = stride\n",
    "    )\n",
    "\n",
    "    # create identity block\n",
    "    block_name_ordinal = ord(\"b\")\n",
    "    for _ in range(layers - 1):\n",
    "        X = block(\n",
    "            X,\n",
    "            kernel_size = kernel_size,\n",
    "            filters =  filters,\n",
    "            stage_no = stage_no,\n",
    "            block_name = chr(block_name_ordinal)\n",
    "        )\n",
    "        block_name_ordinal += 1\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10e63e-5ddf-4ec4-9d77-d4062887aa38",
   "metadata": {},
   "source": [
    "# VGGNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e68076-938f-462a-bcd0-c4e1a4f7cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGNet(name: str, architecture: typing.List[ typing.Union[int, str] ], input_shape: typing.Tuple[int], classes: int = 1000) -> Model:\n",
    "    \"\"\"\n",
    "    Implementation of the VGGNet architecture.\n",
    "    Arguments:\n",
    "    name         -- name of the architecture\n",
    "    architecture -- number of output channel per convolution layers in VGGNet\n",
    "    input_shape  -- shape of the images of the dataset\n",
    "    classes      -- integer, number of classes\n",
    "    Returns:\n",
    "    model        -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # convert input shape into tensor\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # make convolution layers\n",
    "    X = make_conv_layer(X_input, architecture)\n",
    "\n",
    "    # flatten the output and make fully connected layers\n",
    "    X = Flatten()(X)\n",
    "    X = make_dense_layer(X, 4096)\n",
    "    X = make_dense_layer(X, 4096)\n",
    "\n",
    "    # classification layer\n",
    "    X = Dense(units = classes, activation = \"softmax\")(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a4576-8d4c-472e-bfdb-0c50f18f9689",
   "metadata": {},
   "source": [
    "# Make convolutional layer function for VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fec97-2cea-4a17-ba8a-f9c048b8018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv_layer(X: tf.Tensor, architecture: typing.List[ typing.Union[int, str] ], activation: str = 'relu') -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Method to create convolution layers for VGGNet.\n",
    "    In VGGNet\n",
    "        - Kernal is always 3x3 for conv-layer with padding 1 and stride 1.\n",
    "        - 2x2 kernel for max pooling with stride of 2.\n",
    "    Arguments:\n",
    "    X            -- input tensor\n",
    "    architecture -- number of output channel per convolution layers in VGGNet\n",
    "    activation   -- type of activation method\n",
    "    Returns:\n",
    "    X           -- output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    for output in architecture:\n",
    "\n",
    "        # convolution layer\n",
    "        if type(output) == int:\n",
    "            out_channels = output\n",
    "\n",
    "            X = Conv2D(\n",
    "                filters = out_channels,\n",
    "                kernel_size = (3, 3),\n",
    "                strides = (1, 1),\n",
    "                padding = \"same\"\n",
    "            )(X)\n",
    "            X = BatchNormalization()(X)\n",
    "            X = Activation(activation)(X)\n",
    "\n",
    "            # relu activation is added (by default activation) so that all the\n",
    "            # negative values are not passed to the next layer\n",
    "\n",
    "        # max-pooling layer\n",
    "        else:\n",
    "            X = MaxPooling2D(\n",
    "                pool_size = (2, 2),\n",
    "                strides = (2, 2)\n",
    "            )(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd40a6e-c5c7-449f-aa3c-bb6743b4a7fa",
   "metadata": {},
   "source": [
    "# Make dense layer function for VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cd19e-b055-4699-89bb-788cd8e4a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_layer(X: tf.Tensor, output_units: int, dropout = 0.5, activation = 'relu') -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Method to create dense layer for VGGNet.\n",
    "    Arguments:\n",
    "    X            -- input tensor\n",
    "    output_units -- output tensor size\n",
    "    dropout      -- dropout value for regularization\n",
    "    activation   -- type of activation method\n",
    "    Returns:\n",
    "    X            -- input tensor\n",
    "    \"\"\"\n",
    "\n",
    "    X = Dense(units = output_units)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51bcfe-0668-4308-b2fa-fc4dfed6629e",
   "metadata": {},
   "source": [
    "# Custom F1 score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762c025-2406-42a6-9f3f-860a248a182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_f1(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        recall = TP / (Positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "        precision = TP / (Pred_Positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228894-83a2-4f01-8259-07b85f07d87d",
   "metadata": {},
   "source": [
    "# Function for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a989f-2812-4072-a503-93f7f75f1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_models(size1 = 0, width = 28, heigth = 28, channel = 1, num_ep = 100, Epochs = 200, model_name = \"LeNet5\", dataset = 'intents.json'):\n",
    "    start_time1 = time.time()\n",
    "    bag, class_pattern = preprocess(dataset)\n",
    "\n",
    "    size2 = len(bag)\n",
    "\n",
    "    NAME = model_name+\"-test-{}\".format(int(Epochs))\n",
    "    tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "    x_2, y_train, model_x, num_of_classes = train_model(bag, class_pattern, size1, size2, width, heigth, channel, num_ep)\n",
    "    end_time1 = time.time()\n",
    "    model_x.save('Doc2Vec_'+model_name+'-test.h5')\n",
    "    pickle.dump(num_of_classes, open('num_of_classes_'+model_name+'-test.pkl', 'wb'))\n",
    "    max_lebels = max(y_train)\n",
    "\n",
    "    X_train, X_test, y_train2, y_test = train_test_split(x_2, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "    y_train2 = np.array(y_train2)\n",
    "    X_train = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    y_train2 = np.pad(y_train2, (0,0), 'constant')\n",
    "\n",
    "    if model_name == \"LeNet5\":\n",
    "        model = LeNet_5(max_lebels+1)\n",
    "    elif model_name == \"AlexNet\":\n",
    "        model = AlexNet(max_lebels + 1)\n",
    "    elif model_name == \"ResNet\":\n",
    "        model = ResNet(name=\"Resnet50\", layers=[3, 4, 6, 3], input_shape=(224, 224, 3), classes=max_lebels + 1)\n",
    "    elif model_name == \"VGGNet\":\n",
    "        VGG_types = {\n",
    "            'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "            'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512,\n",
    "                      'M'],\n",
    "        }\n",
    "        model = VGGNet(name = \"VGGNet16\", architecture = VGG_types[\"VGG16\"], input_shape=(224, 224, 3), classes = max_lebels+1)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics = ['accuracy', custom_f1])\n",
    "    #model.summary()\n",
    "\n",
    "    hist = model.fit(X_train, y_train2, epochs=Epochs, validation_split=0.3, callbacks=[tensorboard])\n",
    "    model.save('chatbotmodel_'+model_name+'-test.h5', hist)\n",
    "    print('Done')\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    X_test = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    y_test = np.pad(y_test, (0,0), 'constant')\n",
    "\n",
    "    str_path1 = os.path.join(str_path, \"LeNet5_eval\"+NAME+\".txt\")\n",
    "    f2 = open(str_path1, \"w\", encoding = \"utf-8\")\n",
    "    res3 = model.evaluate(X_test, y_test, batch_size=150)\n",
    "    f2.write(\"--\"+NAME+\"\\n\")\n",
    "    f2.write(\"Preprocess : \"+str(end_time1-start_time1)+\"\\n\")\n",
    "    f2.write(str(res3)+\"\\n\")\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b0cc4-76f5-4cba-9d0e-fff1a14e3cf6",
   "metadata": {},
   "source": [
    "# Function for chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7e5ab-2009-40f8-ac8a-a324784d250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(model_name = \"LeNet5\", width = 28, heigth = 28, channel = 1, dataset = 'intents.json', mode = 'manual', questions = [\"Hi\", \"How are you\", \"Goodbye\"]):\n",
    "    model_x = Doc2Vec.load('Doc2Vec_'+model_name+'-test.h5')\n",
    "    num_of_classes = pickle.load(open('num_of_classes_'+model_name+'-test.pkl', 'rb'))\n",
    "    model = load_model('chatbotmodel_'+model_name+'-test.h5', custom_objects={\"custom_f1\": custom_f1})\n",
    "\n",
    "    intents = json.loads(open(dataset, encoding='utf-8').read())\n",
    "    print(\"Chatbot in running. Please ask a question\")\n",
    "\n",
    "    if mode == \"auto\":\n",
    "        for i in range(len(questions)):\n",
    "            res = (model_x.infer_vector(questions[i].split()))\n",
    "            print(\"Me : \"+questions[i])  #Question\n",
    "\n",
    "            x = np.reshape(res, (1,width, heigth, channel))\n",
    "            x2 = np.pad(x, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "            res2 = model.predict(x2)\n",
    "            #print(str(np.argmax(res2))+\" - \"+str(np.max(res2)))  # Number of class - Probability\n",
    "\n",
    "            if np.max(res2) > 0.25:\n",
    "                #print(num_of_classes[np.argmax(res2)])  #Class or Tag\n",
    "                tag = num_of_classes[np.argmax(res2)]\n",
    "                list_of_inttents = intents['intents']\n",
    "                for i in list_of_inttents:\n",
    "                    if i['tag'] == tag:\n",
    "                        print(\"Chatbot : \"+random.choice(i['responses']))  #Response\n",
    "            else :\n",
    "                print(\"I don't understand!\")\n",
    "    elif mode == \"manual\":\n",
    "        stop = False\n",
    "        while not stop:\n",
    "            message = input(\"Me : \")\n",
    "            if message == 'STOP':\n",
    "                stop = True\n",
    "            else:\n",
    "                tokenized_message = nltk.word_tokenize(message)\n",
    "                print(nltk.word_tokenize(message))\n",
    "\n",
    "                res = (model_x.infer_vector(tokenized_message))\n",
    "                #print(\"Me : \" + questions[i])  # Question\n",
    "\n",
    "                x = np.reshape(res, (1, width, heigth, channel))\n",
    "                x2 = np.pad(x, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "\n",
    "                res2 = model.predict(x2)\n",
    "                # print(str(np.argmax(res2))+\" - \"+str(np.max(res2)))  # Number of class - Probability\n",
    "\n",
    "                if np.max(res2) > 0.25:\n",
    "                    # print(num_of_classes[np.argmax(res2)])  #Class or Tag\n",
    "                    tag = num_of_classes[np.argmax(res2)]\n",
    "                    list_of_inttents = intents['intents']\n",
    "                    for i in list_of_inttents:\n",
    "                        if i['tag'] == tag:\n",
    "                            print(\"Chatbot : \" + random.choice(i['responses']))  # Response\n",
    "                else:\n",
    "                    print(\"I don't understand!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58de21-d134-4481-9f76-fd54af16df74",
   "metadata": {},
   "source": [
    "# Timing the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25eca95-9145-4f0f-86bd-aeb2cec63490",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df700356-cc8d-4bda-8a59-53387532ce78",
   "metadata": {},
   "source": [
    "# Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d3a7c-27a7-4cdc-9c73-c1e273ce1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag, class_pattern = preprocess('intents.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b51e23-0acc-4391-aaec-5512a39dabff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e17f8-9565-4090-acbf-73b3b0f962df",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe061d-b527-4694-b31e-2d74ee58d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models(size1=0, width=28, heigth=28, channel=1, num_ep=100, Epochs=200, model_name=\"LeNet5\", dataset='intents.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871ae91-c9e4-4665-b85f-badcede6a57b",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa33d9-2c81-473b-a243-7f6190f877ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models(size1=0, width=223, heigth=223, channel=3, num_ep=100, Epochs=200, model_name=\"AlexNet\", dataset='intents.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c66c36-9136-4728-b5d7-cc3c37b399c5",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097688c6-322c-4c6a-9b40-7e108877940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models(size1=0, width=220, heigth=220, channel=3, num_ep=100, Epochs=15, model_name=\"ResNet\", dataset='intents.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fada5a3-eaad-4b68-b526-3b8de8c3325a",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657602f-9d88-4b5f-aa4f-b08545519ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models(size1=0, width=220, heigth=220, channel=3, num_ep=100, Epochs=15, model_name=\"VGGNet\", dataset='intents.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d271d7e-82f0-4084-a0bd-199482524c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total execution time: {time.time() - start_time1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34e8d0-35f3-42d3-9fa0-da82d06c8d84",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b70dce-89be-48b0-b07b-993edc994488",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690972d-889a-4020-acd5-da5e699e7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot(model_name=\"LeNet5\", width=28, heigth=28, channel=1, dataset='intents.json', mode='manual', questions=[\"Hi\", \"How are you\", \"Goodbye\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fa864-983e-4015-b7e5-a7906d7b34bf",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8abdee-c85d-4de7-8857-c1384fc0a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot(model_name=\"AlexNet\", width=223, heigth=223, channel=3, dataset='intents.json', mode='manual', questions=[\"Hi\", \"How are you\", \"Goodbye\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faf778-b624-4084-b0ab-f76b0952cf38",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89342f8-1a81-4e6c-9327-a2eb2ba45adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot(model_name=\"ResNet\", width=220, heigth=220, channel=3, dataset='intents.json', mode='manual', questions=[\"Hi\", \"How are you\", \"Goodbye\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f294695-efe2-4edc-a18f-fa906ccaf167",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da34bfa-38a7-4c27-a3b2-522afc01d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot(model_name=\"VGGNet\", width=220, heigth=220, channel=3, dataset='intents.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
